{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\", model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'neutral', 'score': 0.8640334010124207}]\n"
     ]
    }
   ],
   "source": [
    "res = nlp(\"DOGECOIN TO THE MOOOOONNNNNNN!!!!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['I like mangos', 'NVDA Shares Soar with higher than expected earnings','Tesla stocks bullish after elon musk announces that he will take over mars',\"Amazon Bearish after Jeff Bezos anounces his career as a physicist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'neutral', 'score': 0.9101503491401672}], [{'label': 'positive', 'score': 0.9478864073753357}], [{'label': 'positive', 'score': 0.5165778994560242}], [{'label': 'negative', 'score': 0.8729503750801086}]]\n"
     ]
    }
   ],
   "source": [
    "test_sents = []\n",
    "for string in tests:\n",
    "    test_sents.append(nlp(string))\n",
    "print(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-06 00:00:00-04:00</th>\n",
       "      <td>329.390516</td>\n",
       "      <td>334.025613</td>\n",
       "      <td>325.454690</td>\n",
       "      <td>333.286407</td>\n",
       "      <td>26443000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-07 00:00:00-04:00</th>\n",
       "      <td>336.642816</td>\n",
       "      <td>338.480862</td>\n",
       "      <td>328.631327</td>\n",
       "      <td>328.871063</td>\n",
       "      <td>28307500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-08 00:00:00-04:00</th>\n",
       "      <td>331.158639</td>\n",
       "      <td>333.046620</td>\n",
       "      <td>328.361589</td>\n",
       "      <td>329.700165</td>\n",
       "      <td>15946100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 00:00:00-04:00</th>\n",
       "      <td>327.282754</td>\n",
       "      <td>330.109741</td>\n",
       "      <td>324.965206</td>\n",
       "      <td>325.105072</td>\n",
       "      <td>14708200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-12 00:00:00-04:00</th>\n",
       "      <td>322.687613</td>\n",
       "      <td>324.036188</td>\n",
       "      <td>317.033608</td>\n",
       "      <td>323.426819</td>\n",
       "      <td>31658700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2021-10-06 00:00:00-04:00  329.390516  334.025613  325.454690  333.286407   \n",
       "2021-10-07 00:00:00-04:00  336.642816  338.480862  328.631327  328.871063   \n",
       "2021-10-08 00:00:00-04:00  331.158639  333.046620  328.361589  329.700165   \n",
       "2021-10-11 00:00:00-04:00  327.282754  330.109741  324.965206  325.105072   \n",
       "2021-10-12 00:00:00-04:00  322.687613  324.036188  317.033608  323.426819   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2021-10-06 00:00:00-04:00  26443000        0.0           0.0  \n",
       "2021-10-07 00:00:00-04:00  28307500        0.0           0.0  \n",
       "2021-10-08 00:00:00-04:00  15946100        0.0           0.0  \n",
       "2021-10-11 00:00:00-04:00  14708200        0.0           0.0  \n",
       "2021-10-12 00:00:00-04:00  31658700        0.0           0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "ticker = yf.Ticker('META')\n",
    "meta = ticker.history('30mo')\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_scraper(input):\n",
    "    finviz_url = \"https://finviz.com/quote.ashx?t=\"\n",
    "    ticker = input\n",
    "\n",
    "    news_tables = {}\n",
    "\n",
    "    url = finviz_url + ticker\n",
    "\n",
    "    req = Request(url = url, headers = {'user-agent': 'mega-chonk'})\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = BeautifulSoup(response, 'html')\n",
    "    news_table = html.find(id = 'news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "        \n",
    "    parsed_data = []\n",
    "\n",
    "    for ticker, news_table in news_tables.items():\n",
    "        for row in news_table.findAll('tr'):\n",
    "            title = row.a.text\n",
    "            date_data = row.td.text\n",
    "            date_data = re.sub(r\"\\r\\n\", \"\", date_data)\n",
    "            date_data = re.sub(r\"\\s+\", \" \", date_data)\n",
    "            date_data = re.sub(r\"^\\s|\\s$\", \"\", date_data)\n",
    "            date_data = date_data.split(\" \")\n",
    "\n",
    "            if date_data[0].lower() == 'today':\n",
    "                date_data[0]= date.today()\n",
    "                time = date_data[1]\n",
    "            if len(date_data) == 1:\n",
    "                time = date_data[0]\n",
    "            else:\n",
    "                date_article = date_data[0]\n",
    "                time = date_data[1]\n",
    "\n",
    "            parsed_data.append([ticker, date_article, time, title])\n",
    "\n",
    "    # print(parsed_data)\n",
    "\n",
    "    df = pd.DataFrame(parsed_data, columns = ['ticker', 'date', 'time', 'title'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Analyzer:\n",
    "    def __init__(self, weighting, model, tokenizer):\n",
    "        self.weight = weighting\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.nlp = pipeline(\"sentiment-analysis\",model = self.model, tokenizer = self.tokenizer)\n",
    "    def sentiment_label(self, title, nlp):\n",
    "        results = nlp(title)\n",
    "        label = results[0]['label']\n",
    "        return label\n",
    "    def sentiment_score(self, title, nlp):\n",
    "        results = nlp(title)\n",
    "        score= results[0]['score']\n",
    "        return score\n",
    "    def analyzer(self, df, ticker):\n",
    "        df['Sentiment Label'] = df['title'].apply(lambda title: self.sentiment_label(title, self.nlp))\n",
    "        df['Sentiment Score'] = df['title'].apply(lambda title: self.sentiment_score(title, self.nlp))\n",
    "        net_sent_score = 0\n",
    "        for row in range(df.shape[0]):\n",
    "            if df.iloc[row]['Sentiment Label'] == 'negative':\n",
    "                net_sent_score -= df.iloc[row]['Sentiment Score']\n",
    "            elif df.iloc[row]['Sentiment Label'] == 'positive':\n",
    "                net_sent_score += df.iloc[row]['Sentiment Score']\n",
    "            else:\n",
    "                pass\n",
    "        net_sent_score = net_sent_score / df.shape[0]\n",
    "        return self.weight*net_sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = Sentiment_Analyzer(1, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'neutral', 'score': 0.8766472935676575}]\n"
     ]
    }
   ],
   "source": [
    "mung = finbert.nlp('I love amongus')\n",
    "print(mung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = ['NVDA', 'MSFT', 'ORCL', 'TSM', 'AVGO', 'ASML', 'AMD', 'CRM', 'ADBE', 'AAPL',\n",
    "               'JPM', 'BAC', 'WFC', 'AXP', 'BX', 'MS', 'HSBC', 'RY', 'MA', 'V',\n",
    "               'LLY', 'NVO', 'UNH', 'JNJ', 'ABBV', 'MRK', 'TMO', 'AZN', 'ABT', 'NVS',\n",
    "                'GE', 'CAT', 'UNP', 'UPS', 'HON','RTX','ETN','DE','BA','LMT',\n",
    "                'AMZN', 'TSLA', 'HD', 'TM', 'MCD','BABA','PDD','LOW','NKE','BKNG',\n",
    "                'LIN', 'BHP', 'RIO', 'SHW', 'SCCO','FCX','ECL','CRH','APD','VALE',\n",
    "                'PLD', 'AMT', 'EQIX', 'SPG', 'WELL','PSA','O','DLR','CCI','CSGP',\n",
    "                'GOOGL', 'META', 'NFLX', 'DIS', 'TMUS','VZ','CMCSA','T','RELX','SPOT',\n",
    "                'XOM', 'CVX', 'SHEL', 'TTE', 'COP','BP','PBR','CNQ','EQNR','MPC',\n",
    "                'WMT', 'PG', 'COST', 'KO', 'PEP','PM','BUD','UL','MDLZ','DEO',\n",
    "                'NEE', 'SO', 'DUK', 'CEG', 'NGG','SRE','AEP','D','EXC','PCG',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006030586957931519"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'META'\n",
    "finbert.analyzer(web_scraper(ticker), ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dis_finbert = pd.DataFrame({'Score':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA: -0.01930225133895874\n",
      "MSFT: -0.11872526288032531\n",
      "ORCL: 0.1320860430598259\n",
      "TSM: -0.04832781821489334\n",
      "AVGO: 0.21928513050079346\n",
      "ASML: 0.0868469911813736\n",
      "AMD: -0.16890289962291719\n",
      "CRM: 0.046415905952453616\n",
      "ADBE: -0.0871883600950241\n",
      "AAPL: -0.16058723896741867\n",
      "JPM: -0.0555382364988327\n",
      "BAC: -0.0920705395936966\n",
      "WFC: -0.08290811449289322\n",
      "AXP: 0.10253255486488343\n",
      "BX: 0.02627530097961426\n",
      "MS: -0.0172734397649765\n",
      "HSBC: -0.013246671557426452\n",
      "RY: -0.012691229283809662\n",
      "MA: 0.1674931412935257\n",
      "V: 0.06696222484111786\n",
      "LLY: -0.04775205761194229\n",
      "NVO: 0.10153740137815476\n",
      "UNH: -0.2161150985956192\n",
      "JNJ: 0.09514287412166596\n",
      "ABBV: 0.10496597945690155\n",
      "MRK: 0.26212720096111297\n",
      "TMO: -0.019492469429969787\n",
      "AZN: 0.2880526149272919\n",
      "ABT: 0.1524798008799553\n",
      "NVS: -0.016481006145477296\n",
      "GE: 0.050961278080940246\n",
      "CAT: 0.03500261336565018\n",
      "UNP: -0.13786893367767333\n",
      "UPS: 0.11716000616550445\n",
      "HON: 0.09471730291843414\n",
      "RTX: 0.26571345269680025\n",
      "ETN: 0.24600686967372895\n",
      "DE: -0.14783135592937469\n",
      "BA: -0.2739456620812416\n",
      "LMT: 0.23337339282035827\n",
      "AMZN: -0.059122367203235625\n",
      "TSLA: -0.11224366933107376\n",
      "HD: 0.06283923357725144\n",
      "TM: 0.041675757765769955\n",
      "MCD: -0.0015548157691955566\n",
      "BABA: -0.004952134191989898\n",
      "PDD: -0.03662267476320267\n",
      "LOW: -0.038814302384853366\n",
      "NKE: -0.1641064515709877\n",
      "BKNG: 0.09819535404443741\n",
      "LIN: 0.13885412186384202\n",
      "BHP: -0.09774608045816421\n",
      "RIO: -0.0016482913494110108\n",
      "SHW: 0.10162582963705064\n",
      "SCCO: 0.11214594483375549\n",
      "FCX: 0.019834022521972656\n",
      "ECL: 0.2444167935848236\n",
      "CRH: 0.1207945266366005\n",
      "APD: 0.06051697492599487\n",
      "VALE: 0.043078030049800875\n",
      "PLD: 0.11073445469141006\n",
      "AMT: 0.01626598298549652\n",
      "EQIX: 0.009484748542308807\n",
      "SPG: 0.09549462705850602\n",
      "WELL: 0.10370263397693634\n",
      "PSA: 0.08688583940267564\n",
      "O: 0.01843829721212387\n",
      "DLR: 0.10698748916387558\n",
      "CCI: -0.030832867622375488\n",
      "CSGP: 0.047639780044555664\n",
      "GOOGL: -0.02411889523267746\n",
      "META: 0.0028690582513809203\n",
      "NFLX: -0.03200689435005188\n",
      "DIS: -0.14889944165945054\n",
      "TMUS: 0.06514210969209672\n",
      "VZ: 0.0941261026263237\n",
      "CMCSA: 0.06542769670486451\n",
      "T: -0.05890963017940521\n",
      "RELX: 0.2990639618039131\n",
      "SPOT: 0.04910166829824448\n",
      "XOM: 0.004420127272605896\n",
      "CVX: 0.04056091189384461\n",
      "SHEL: -0.004253992438316345\n",
      "TTE: 0.19617624461650848\n",
      "COP: 0.0421579173207283\n",
      "BP: 0.04431625872850418\n",
      "PBR: -0.0027532312273979187\n",
      "CNQ: 0.009040833115577698\n",
      "EQNR: 0.1798609334230423\n",
      "MPC: 0.09763589382171631\n",
      "WMT: 0.05642827808856964\n",
      "PG: -0.01931717187166214\n",
      "COST: 0.019689258933067322\n",
      "KO: 0.019630148708820343\n",
      "PEP: 0.013563300967216491\n",
      "PM: -0.007651378512382507\n",
      "BUD: -0.030451527833938598\n",
      "UL: -0.18926295787096023\n",
      "MDLZ: 0.09354734063148498\n",
      "DEO: 0.005184612870216369\n",
      "NEE: 0.0049649238586425785\n",
      "SO: 0.18405704259872435\n",
      "DUK: 0.24709513545036316\n",
      "CEG: 0.19883001685142518\n",
      "NGG: 0.16065755724906922\n",
      "SRE: 0.1515321707725525\n",
      "AEP: 0.2084030681848526\n",
      "D: 0.03423792898654938\n",
      "EXC: 0.24941728085279466\n",
      "PCG: 0.15096128165721892\n"
     ]
    }
   ],
   "source": [
    "for symbol in ticker_list:\n",
    "    score = finbert.analyzer(web_scraper(symbol),symbol)\n",
    "    new_row = {'Score':score}\n",
    "    score_dis_finbert.loc[len(score_dis_finbert)] = new_row\n",
    "    print(f'{symbol}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = Sentiment_Analyzer(1, model_bert, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dis_bert = pd.DataFrame({'Score':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.29767200350761414}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.nlp('mungus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA: 0.0\n",
      "MSFT: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m ticker_list:\n\u001b[1;32m----> 2\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweb_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m:score}\n\u001b[0;32m      4\u001b[0m     score_dis_bert\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(score_dis_bert)] \u001b[38;5;241m=\u001b[39m new_row\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mSentiment_Analyzer.analyzer\u001b[1;34m(self, df, ticker)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyzer\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, ticker):\n\u001b[1;32m---> 16\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment Label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m title: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_score(title, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp))\n\u001b[0;32m     18\u001b[0m     net_sent_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mSentiment_Analyzer.analyzer.<locals>.<lambda>\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyzer\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, ticker):\n\u001b[1;32m---> 16\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment Label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m title: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m title: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_score(title, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp))\n\u001b[0;32m     18\u001b[0m     net_sent_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mSentiment_Analyzer.sentiment_label\u001b[1;34m(self, title, nlp)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentiment_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, title, nlp):\n\u001b[1;32m----> 8\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     label \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1122\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1123\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         )\n\u001b[0;32m   1127\u001b[0m     )\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1136\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1135\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1136\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1035\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1034\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1035\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:367\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m    365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[1;32m--> 367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n\u001b[0;32m    369\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mview(new_context_layer_shape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for symbol in ticker_list:\n",
    "    score = bert.analyzer(web_scraper(symbol),symbol)\n",
    "    new_row = {'Score':score}\n",
    "    score_dis_bert.loc[len(score_dis_bert)] = new_row\n",
    "    print(f'{symbol}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(score_dis_finbert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
